app {
  #application name
  name: "FBApiDataPipeline"
  #blossom id of th project. Get blossom id from blossom application
  blossomID: "CI02995445",
  #team name
  teamName: "DataSciences-DE-MDF-TM"
  #Contact details
  contactDetails: "DataSciences-DE-MDF-TM@Target.com",
  channel:"#mdf-success-mgs",
  env: "local",
  sla: "11:00 AM"

  queueName="SVMDEDMD_AUTO"
  executionEngine="tez"
  dynamicPartition="true"
  dynamicPartitionMode="nonstrict"
  jdbcUrl="jdbc:hive2://brcn1003.target.com:2181,brcn1004.target.com:2181,brcn1008.target.com:2181,brcn1009.target.com:2181,brcn1012.target.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2"
  hiveTezInputFormat="org.apache.hadoop.hive.ql.io.HiveInputFormat"
  mapReduceJobReduces="50"
  jdbcHiveDriver="org.apache.hive.jdbc.HiveDriver"



  ## Input Locations
  #location of the template file for comparing schema of messages
  templateLocation = "src/test/data/data-processing-pipeline/template/template.json/",
  #location from where input messages can be read
  inputLocation ="src/test/data/data-processing-pipeline/processing",
  #dateparm file location
  #dateparmLocation ="/user/SVMDEDMD/landing/gam_api_actuals/dateparm/gam_api_gregd.dat",
  #calednarLocation ="/user/SVSGNHDS/hive/calendar/",

  fileAPILandingLocation ="src/test/data/data-processing-pipeline/land/",
  existingRecordsLocation ="src/test/data/data-processing-pipeline/archive/",
  archivePartitonRecordsLocation ="src/test/data/data-processing-pipeline/archive_partition/",
  inputFileFormat = "json"
  templateFileFormat = "json"

  #Check point path for fetching last processed load date partition
  #checkPointPath= "/user/SVMDEDMD/landing/gam_api_actuals/configurations/dpp_processed_checkpoint" ##del?
  #templndExtLocation = "/user/SVMDEDMD/hive/gam_api_actuals/gam_actuals_land_e_ext_tmp/",
  tempFNDExtLocation = "/user/SVMDEDMD/hive/fb_api_actuals/stage",
  corruptRecordLocation = "src/test/data/data-processing-pipeline/output/pipeLineFailures/corruptRecords"


  numberOfPartitions=3

  influxCustomMeasurementName= "mdf_table_partition_details_stg"
  #readAlpaca="true"

  ## Output Locations
  #final output of the records
  #outputLocation="/user/SVMDEDMD/landing/gam_api_actuals/data_processing_pipeline/output/mergedData"
  # Failure location
  pipeLineFailures="src/test/data/data-processing-pipeline/output/pipeLineFailures"
  #location where lock will be created, so that user cannot rerun job if the lock is present
  lockLocation="src/test/data/data-processing-pipeline/"

  #mergeOutputLocation = "hdfs://bigred3ns/user/SVMDEDMD/landing/gam_api_actuals/data_processing_pipeline/output/mergedData"
  #stageOutputLocation = "hdfs://bigred3ns/user/SVMDEDMD/landing/stores_presentation_sign/data_processing_pipeline/output/stagingData"
  #coreHistLocationPath="/user/SVMDEDMD/landing/gam_api_actuals/data_processing_pipeline/output/coreHistData"
  fileFormat="orc"
  dataQualityLocation="src/test/data/data-processing-pipeline/output/pipeLineFailures"


  #spark session creation properties
  spark_sources_partitionOverwriteMode="static"
  hive_partition_mode="nonstrict"
  spark_orc_impl="native"
  spark_enableVectorizedReader="true"
  spark_wareHouseLocation="/apps/hive/warehouse"
  hive_max_dynamic_partitions="2000"
  dqThreshold="1"
  dq_channelID="0"
  alert_channelID="0"
  errChannel="0"

  #DB schema names
  #calDateDimSchema ="prd_cal_fnd"
  fndSchema = "stg_mdf_fnd"
  #stgSchema = "stg_sgn_lzn"
  lndSchema = "stg_mdf_lzn"
  wrkSchema = "stg_mdf_tmp"

  #locationSchema = "prd_loc_fnd"
  #coreItemSchema ="dev_itemxx_stg"
  #strTasksEventsSchema ="dev_item_fnd"
  #strTasksEventsStgSchema="dev_item_stg"


  #DB table names
  lndCoreHistTable ="fb_api_actuals_e"
  coreHistTable ="fb_api_actuals"
  #lndExtTable ="store_presentation_sign_events_e_ext"
  fndExtTable ="fb_api_actuals_stg"
  #calDateDimTable ="calendar"

  coreHistPartitionColumnName="report_d"
  atomicHistPartitionColumnName="report_d"


  #coalesce to be done for each stage. For demo we are taking this as global value.
  # Value should be calculated based on expected load
  coalesceFactor=1

  #numberOfPartitions
  #the max partitions allowed to be written into the path provided
  numberOfPartitions=10
  influxdb {
      #measurement name got from measurement team
      measurementName = "fb-api-actuals-dpp-stg"
      #end point of influx to push data
      grafanaEndPoint = "https://metricsfrontdoor-shared.prod.target.com/write?db=metrics"
  }

  metrics {
        metricsKafkaPush=false
        metricsInfluxPush=true
  }

  timeliness {
    jceksFilePath="jceks://hdfs/common/SVMDEDMP/ssl_certs/mdf_secrets.jceks"
    truststoreFilePath="/common/SVMDEDMP/ssl_certs/client.truststore.jks"
    keystoreFilePath="/common/SVMDEDMP/ssl_certs_mdf-prd.target.com.jks"
  }

  dataportal {
        sm_user = "SVMDEDMD"
        sm_name = "Chris.mewes@target.com"
        dataTrustURL = "https://services.datatrust.target.com/datatrust-service/validate"
        timelinessURL = "https://services.dataportal.target.com/event-ingest-service/event-service/timeliness/event/dataMovement"
        lineageURL = "https://services.dataportal.target.com/metadata-lineage-service/lineage-service/lineage/create"
        objectUUID = "4e6b0a30-43b9-11ea-b5ac-51cfba7cfc4fstg_mdf_fndfb_api_actuals_stg"
        objectUUIDWF = "5052cbd0-b49d-11ea-bc24-51cfba7cfc4f2130869-210217125056108-oozie-oozi-c"
        metricName = "CountMetric"
        ruleType = "GREATERTHAN"
        dpPushRetryCount = "2"
        dqThreshold = "1"
        commoningestuuid = "4e6b0a30-43b9-11ea-b5ac-51cfba7cfc4fstg_mdf_fndfb_api_actuals_stg"
        addLineage_flag = "false"
  }

}